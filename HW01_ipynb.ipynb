{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoondaeng/ICE4104-AI-Applications/blob/main/HW01_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HW01: Regression, Cross-Validation, and Regularization\n",
        "\n",
        "아래 코드 문제들을 풀고, 설명을 작성하시오."
      ],
      "metadata": {
        "id": "AD7QnvqhfNZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Task 1: Implement `calc_root_mean_squared_error`\n",
        "\n",
        "See the test cases below and complete `calc_root_mean_squared_error`."
      ],
      "metadata": {
        "id": "fUA_AFXcfMkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Test Cases\n",
        "--------\n",
        ">>> y_N = 0.0\n",
        ">>> yhat_N = 4.123\n",
        ">>> calc_root_mean_squared_error(y_N, yhat_N)\n",
        "4.123\n",
        "\n",
        ">>> y_N = np.asarray([-2, 0, 2], dtype=np.float64)\n",
        ">>> yhat_N = np.asarray([-4, 0, 2], dtype=np.float64)\n",
        ">>> rmse = calc_root_mean_squared_error(y_N, yhat_N)\n",
        ">>> np.round(rmse, 6)\n",
        "1.154701\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def calc_root_mean_squared_error(y_N, yhat_N):\n",
        "    ''' Compute root mean squared error given true and predicted values\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "    y_N : 1D array, shape (N,)\n",
        "        Each entry represents 'ground truth' numeric response for an example\n",
        "    yhat_N : 1D array, shape (N,)\n",
        "        Each entry representes predicted numeric response for an example\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rmse : scalar float\n",
        "        Root mean squared error performance metric\n",
        "        .. math:\n",
        "            rmse(y,\\hat{y}) = \\sqrt{\\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2}\n",
        "    '''\n",
        "    y_N = np.atleast_1d(y_N) # 입력값을 최소 1차원 배열로 변환 -> 입력이 스칼라인 경우에도 허용\n",
        "    yhat_N = np.atleast_1d(yhat_N)\n",
        "    assert y_N.ndim == 1 # y_N이 1차원 배열인지 확인\n",
        "    assert y_N.shape == yhat_N.shape # 예측값과 실제값의 형태가 동일한지 확인 -> 두 배열의 길이 동일\n",
        "    mse = np.mean((y_N - yhat_N) ** 2) # 평균 제곱 오차(MSE)를 계산\n",
        "    rmse = np.sqrt(mse) # 루트를 취하여 RMSE\n",
        "    return rmse"
      ],
      "metadata": {
        "id": "gwp0_QyFf1m-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 케이스\n",
        "y_N = 0.0\n",
        "yhat_N = 4.123\n",
        "calc_root_mean_squared_error(y_N, yhat_N)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCJ4NuJLUbtt",
        "outputId": "9bc3f920-4d6d-4194-9c98-b0b57e67de25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.123"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 케이스 배열로 RMSE 계산\n",
        "y_N = np.asarray([-1, 0, 1], dtype=np.float64)\n",
        "yhat_N = np.asarray([-2, 0, 2], dtype=np.float64)\n",
        "rmse = calc_root_mean_squared_error(y_N, yhat_N)\n",
        "np.round(rmse, 6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u4QtYTaU4pg",
        "outputId": "54a02e62-9184-430a-fcda-7c91cb80f637"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.816497"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ans 1: 코드 설명\n",
        "\n",
        "* 이 코드는 주어진 실제값(`y_N`)과 예측값(`yhat_N`) 간의 루트 평균 제곱 오차(RMSE, Root Mean Squared Error)을 계산하는 함수\n",
        "\n",
        "- 평균 제곱 오차(MSE)에 루트를 취하여 RMSE를 계산한다.\n",
        "\n",
        "- RMSE (Root Mean Squared Error)\n",
        "    - 예측값이 실제값과 얼마나 가까운지를 측정하는 지표\n",
        "    - 값이 작을수록 예측이 실제 값에 가깝다는 것을 의미\n",
        "\n",
        "### 함수 정의 및 매개변수\n",
        "- `y_N`\n",
        "    - 실제값들을 포함하는 1차원 배열\n",
        "- `yhat_N`\n",
        "    - 예측값들을 포함하는 1차원 배열\n",
        "- `Return`\n",
        "    - RMSE는 실제 값과 예측 값 간의 루트 평균 제곱 오차를 나타내는 스칼라(float)\n",
        "\n"
      ],
      "metadata": {
        "id": "3OHsKE9dg1x8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Task 2: Implement `fit` and `predict`\n",
        "\n",
        "This code defines a LeastSquaresLinearRegressor class with the two key methods of the usual sklearn regression API: `fit` and `predict`. You will edit this file to complete the `fit` and the `predict` methods, which will demonstrate your understanding of what goes on \"inside\" sklearn-like regressor objects.\n",
        "\n"
      ],
      "metadata": {
        "id": "8IW_4ciDgOKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2(a):  The fit method should take in a labeled dataset $\\{x_n, y_n\\}_{n=1}^N$ and instantiate two instance attributes\n",
        "\n",
        "* `w_F` : 1D numpy array, shape (n_features = F,) Represents the 'weights' Contains float64 entries of the weight coefficients\n",
        "* `b` : scalar float Represents the 'bias' or 'intercept'.\n",
        "\n",
        "Hint: Within a Python class, you can set an attribute like `self.b = 1.0`.\n",
        "\n",
        "Nothing should be returned. You're updating the internal state of the object.\n",
        "\n",
        "These attributes should be set using the formulas discussed in class (Lecture 03) for solving the \"least squares\" optimization problem (finding w and b values that minimize squared error on the training set).\n"
      ],
      "metadata": {
        "id": "DN1-VzpnjhmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Task 2(b):  The `predict` method should take in an array of feature vectors $\\{x_n\\}^N_{n=1}$ and produce (return) the predicted responses $\\{\\hat{y}(x_n)\\}^N_{n=1}$.\n",
        "\n",
        "Recall that for linear regression, we've defined the prediction function as:\n",
        "\n",
        "$$\\hat{y}(x_n)=b+w^Tx_n=b+\\sum_{f=1}^F{w_f x_{n,f}}$$"
      ],
      "metadata": {
        "id": "ZCLUB7bUmtFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Test Case\n",
        "---------\n",
        ">>> prng = np.random.RandomState(0)\n",
        ">>> N = 100\n",
        "\n",
        ">>> true_w_F = np.asarray([1.1, -2.2, 3.3])\n",
        ">>> true_b = 0.0\n",
        ">>> x_NF = prng.randn(N, 3)\n",
        ">>> y_N = true_b + np.dot(x_NF, true_w_F) + 0.03 * prng.randn(N)\n",
        "\n",
        ">>> linear_regr = LeastSquaresLinearRegressor()\n",
        ">>> linear_regr.fit(x_NF, y_N)\n",
        "\n",
        ">>> yhat_N = linear_regr.predict(x_NF)\n",
        ">>> np.set_printoptions(precision=3, formatter={'float':lambda x: '% .3f' % x})\n",
        ">>> print(linear_regr.w_F)\n",
        "[ 1.099 -2.202  3.301]\n",
        ">>> print(np.asarray([linear_regr.b]))\n",
        "[-0.005]\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "# No other imports allowed!\n",
        "\n",
        "class LeastSquaresLinearRegressor(object):\n",
        "    ''' A linear regression model with sklearn-like API\n",
        "\n",
        "    Fit by solving the \"least squares\" optimization problem.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    * self.w_F : 1D numpy array, size n_features (= F)\n",
        "        vector of weights, one value for each feature\n",
        "    * self.b : float\n",
        "        scalar real-valued bias or \"intercept\"\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        ''' Constructor of an sklearn-like regressor\n",
        "\n",
        "        Should do nothing. Attributes are only set after calling 'fit'.\n",
        "        '''\n",
        "        # Leave this alone\n",
        "        pass\n",
        "\n",
        "    def fit(self, x_NF, y_N):\n",
        "        ''' Compute and store weights that solve least-squares problem.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "        x_NF : 2D numpy array, shape (n_examples, n_features) = (N, F)\n",
        "            Input measurements (\"features\") for all examples in train set.\n",
        "            Each row is a feature vector for one example.\n",
        "        y_N : 1D numpy array, shape (n_examples,) = (N,)\n",
        "            Response measurements for all examples in train set.\n",
        "            Each row is a feature vector for one example.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing.\n",
        "\n",
        "        Post-Condition\n",
        "        --------------\n",
        "        Internal attributes updated:\n",
        "        * self.w_F (vector of weights for each feature)\n",
        "        * self.b (scalar real bias, if desired)\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        The least-squares optimization problem is:\n",
        "\n",
        "        .. math:\n",
        "            \\min_{w \\in \\mathbb{R}^F, b \\in \\mathbb{R}}\n",
        "                \\sum_{n=1}^N (y_n - b - \\sum_f x_{nf} w_f)^2\n",
        "        '''\n",
        "        N, F = x_NF.shape\n",
        "\n",
        "        # 절편을 위한 열 추가\n",
        "        X_aug = np.hstack((np.ones((N, 1)), x_NF))\n",
        "\n",
        "        # 선형 회귀 정규 방정식 -> np.linalg.solve 함수를 이용하여 가중치와 절편 계산\n",
        "        theta = np.linalg.solve(np.dot(X_aug.T, X_aug), np.dot(X_aug.T, y_N))\n",
        "\n",
        "        # 절편 b\n",
        "        self.b = theta[0]\n",
        "        # 가중치\n",
        "        self.w_F = theta[1:]\n",
        "\n",
        "\n",
        "    def predict(self, x_MF):\n",
        "        ''' Make predictions given input features for M examples\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "        x_MF : 2D numpy array, shape (n_examples, n_features) (M, F)\n",
        "            Input measurements (\"features\") for all examples of interest.\n",
        "            Each row is a feature vector for one example.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        yhat_M : 1D array, size M\n",
        "            Each value is the predicted scalar for one example\n",
        "        '''\n",
        "        # 주어진 입력 데이터 x_MF 에 대해 가중치 w_F 와 절편 b을 사용하여 예측값 계산\n",
        "        yhat_M = self.b + np.dot(x_MF, self.w_F)\n",
        "        return yhat_M"
      ],
      "metadata": {
        "id": "NeoNqhZsgWCW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 코드\n",
        "prng = np.random.RandomState(0)\n",
        "N = 100\n",
        "\n",
        "true_w_F = np.asarray([1.1, -2.2, 3.3])\n",
        "true_b = 0.0\n",
        "x_NF = prng.randn(N, 3)\n",
        "y_N = true_b + np.dot(x_NF, true_w_F) + 0.03 * prng.randn(N)\n",
        "\n",
        "linear_regr = LeastSquaresLinearRegressor()\n",
        "linear_regr.fit(x_NF, y_N)\n",
        "\n",
        "yhat_N = linear_regr.predict(x_NF)\n",
        "np.set_printoptions(precision=3, formatter={'float':lambda x: '% .3f' % x})\n",
        "print(linear_regr.w_F)\n",
        "\n",
        "print(np.asarray([linear_regr.b]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJfq1USIWYkA",
        "outputId": "540e320c-bd06-436c-9a16-13fadb4bab62"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.099 -2.202  3.301]\n",
            "[-0.005]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ans 2: 코드 설명\n",
        "\n",
        "* 이 코드는 `LeastSquaresLinearRegressor` 클래스를 정의하여 선형 회귀 모델을 구현한다.\n",
        "- `LeastSquaresLinearRegressor` 클래스는 fit 및 predict 메서드를 통해 데이터를 학습하고 예측한다.\n",
        "- fit 메서드는 모델의 가중치와 절편을 학습하고, predict 메서드는 주어진 입력에 대한 예측 값을 리턴\n",
        "\n",
        "### `init`\n",
        "- 회귀의 생성자\n",
        "- 메서드는 초기화 시 아무런 동작도 하지 않음\n",
        "- 속성은 fit 메서드를 호출한 후에 설정\n",
        "\n",
        "### `fit`\n",
        "- fit 메서드는 주어진 학습 데이터를 사용하여 모델의 가중치(w_F)와 절편(b)을 계산한다.\n",
        "- 입력 행렬 x_NF에 절편을 위한 열을 추가하여 X_aug를 만든다.\n",
        "- 선형 회귀의 정규 방정식을 풀기 위해 np.linalg.solve 함수를 사용하여 가중치와 절편을 계산한다\n",
        "- theta 벡터에서 첫 번째 값은 절편(b)이고 나머지 값들은 가중치(w_F)를 의미한다.\n",
        "\n",
        "### `predict`\n",
        "- 주어진 입력 데이터 x_MF에 대해 가중치(w_F)와 절편(b)을 사용하여 예측값을 계산\n",
        "- 예측값 = 절편(b)과 입력 데이터 x_MF와 가중치(w_F)의 내적의 합"
      ],
      "metadata": {
        "id": "gXvrR5rMhBnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test code\n",
        "def test_on_toy_data(N=100):\n",
        "    '''\n",
        "    Simple example use case\n",
        "    With toy dataset with N=100 examples\n",
        "    created via a known linear regression model plus small noise\n",
        "    '''\n",
        "    prng = np.random.RandomState(0)\n",
        "\n",
        "    true_w_F = np.asarray([1.1, -2.2, 3.3])\n",
        "    true_b = 0.0\n",
        "    x_NF = prng.randn(N, 3)\n",
        "    y_N = true_b + np.dot(x_NF, true_w_F) + 0.03 * prng.randn(N)\n",
        "\n",
        "    linear_regr = LeastSquaresLinearRegressor()\n",
        "    linear_regr.fit(x_NF, y_N)\n",
        "\n",
        "    yhat_N = linear_regr.predict(x_NF)\n",
        "\n",
        "    np.set_printoptions(precision=3, formatter={'float':lambda x: '% .3f' % x})\n",
        "\n",
        "    print(\"True weights\")\n",
        "    print(true_w_F)\n",
        "    print(\"Estimated weights\")\n",
        "    print(linear_regr.w_F)\n",
        "\n",
        "    print(\"True intercept\")\n",
        "    print(np.asarray([true_b]))\n",
        "    print(\"Estimated intercept\")\n",
        "    print(np.asarray([linear_regr.b]))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test_on_toy_data()"
      ],
      "metadata": {
        "id": "GLyiEfjRggYr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0221e5af-5aa3-4bac-9517-bb42eb817c1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True weights\n",
            "[ 1.100 -2.200  3.300]\n",
            "Estimated weights\n",
            "[ 1.099 -2.202  3.301]\n",
            "True intercept\n",
            "[ 0.000]\n",
            "Estimated intercept\n",
            "[-0.005]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Task 3: Randomly divide data into splits and estimate training and heldout error.\n",
        "\n",
        "### Task 3(a) : Implement the `make_train_and_test_row_ids_for_n_fold_cv` function\n",
        "\n",
        "\n",
        "This function should consume the number of examples, the desired number of folds, and a pseudo-random number generator. Then, it will produce, for each of the desired number of folds, arrays of integers indicating which rows of the dataset belong to the training set, and which belong to the test set.\n",
        "\n",
        "\n",
        "See the starter code for detailed specification.\n",
        "\n",
        "Note : For each fold, you do NOT need to produce exactly the same random splits as our code. For instance, while creating 3 fold splits for an array all_examples=[1, 2, 3, 4, 5], the examples in each in fold could be :\n",
        "\n",
        "* fold0_examples=[1, 2]\n",
        "* fold1_examples=[3, 5]\n",
        "* fold2_examples=[4]\n",
        "**OR**\n",
        "* fold0_examples=[3, 4]\n",
        "* fold1_examples=[1, 5]\n",
        "* fold2_examples=[2]\n",
        "\n"
      ],
      "metadata": {
        "id": "_TJpgNsulV0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_train_and_test_row_ids_for_n_fold_cv(\n",
        "        n_examples=0, n_folds=3, random_state=0):\n",
        "    ''' Divide row ids into train and test sets for n-fold cross validation.\n",
        "\n",
        "    Will *shuffle* the row ids via a pseudorandom number generator before\n",
        "    dividing into folds.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "    n_examples : int\n",
        "        Total number of examples to allocate into train/test sets\n",
        "    n_folds : int\n",
        "        Number of folds requested\n",
        "    random_state : int or numpy RandomState object\n",
        "        Pseudorandom number generator (or seed) for reproducibility\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_ids_per_fold : list of 1D np.arrays\n",
        "        One entry per fold\n",
        "        Each entry is a 1-dim numpy array of unique integers between 0 to N\n",
        "    test_ids_per_fold : list of 1D np.arrays\n",
        "        One entry per fold\n",
        "        Each entry is a 1-dim numpy array of unique integers between 0 to N\n",
        "\n",
        "    Guarantees for Return Values\n",
        "    ----------------------------\n",
        "    Across all folds, guarantee that no two folds put same object in test set.\n",
        "    For each fold f, we need to guarantee:\n",
        "    * The *union* of train_ids_per_fold[f] and test_ids_per_fold[f]\n",
        "    is equal to [0, 1, ... N-1]\n",
        "    * The *intersection* of the two is the empty set\n",
        "    * The total size of train and test ids for any fold is equal to N\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> N = 11\n",
        "    >>> n_folds = 3\n",
        "    >>> tr_ids_per_fold, te_ids_per_fold = (\n",
        "    ...     make_train_and_test_row_ids_for_n_fold_cv(N, n_folds))\n",
        "    >>> len(tr_ids_per_fold)\n",
        "    3\n",
        "\n",
        "    # Count of items in training sets\n",
        "    >>> np.sort([len(tr) for tr in tr_ids_per_fold])\n",
        "    array([7, 7, 8])\n",
        "\n",
        "    # Count of items in the test sets\n",
        "    >>> np.sort([len(te) for te in te_ids_per_fold])\n",
        "    array([3, 4, 4])\n",
        "\n",
        "    # Test ids should uniquely cover the interval [0, N)\n",
        "    >>> np.sort(np.hstack([te_ids_per_fold[f] for f in range(n_folds)]))\n",
        "    array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
        "\n",
        "    # Train ids should cover the interval [0, N) TWICE\n",
        "    >>> np.sort(np.hstack([tr_ids_per_fold[f] for f in range(n_folds)]))\n",
        "    array([ 0,  0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,\n",
        "            8,  9,  9, 10, 10])\n",
        "    '''\n",
        "    if hasattr(random_state, 'rand'):\n",
        "        # Handle case where provided random_state is a random generator\n",
        "        # (e.g. has methods rand() and randn())\n",
        "        random_state = random_state # just remind us we use the passed-in value\n",
        "    else:\n",
        "        # Handle case where we pass \"seed\" for a PRNG as an integer\n",
        "        random_state = np.random.RandomState(int(random_state))\n",
        "\n",
        "    # TODO obtain a shuffled order of the n_examples\n",
        "\n",
        "    # 0부터 n_examples-1까지의 정수 배열 생성\n",
        "    shuffled_indices = np.arange(n_examples)\n",
        "    # shuffle -> 배열을 무작위로 섞음\n",
        "    random_state.shuffle(shuffled_indices)\n",
        "\n",
        "    # fold_size: 각 폴드의 크기를 저장하는 배열\n",
        "    fold_sizes = np.full(n_folds, n_examples // n_folds, dtype=int)\n",
        "    fold_sizes[:n_examples % n_folds] += 1\n",
        "\n",
        "    train_ids_per_fold = list()\n",
        "    test_ids_per_fold = list()\n",
        "\n",
        "    # TODO establish the row ids that belong to each fold's\n",
        "    # train subset and test subset\n",
        "\n",
        "    current = 0 # 현재 위치 추적하는 변수\n",
        "    for fold_size in fold_sizes:\n",
        "        start, stop = current, current + fold_size # 테스트와 학습 데이터 분할\n",
        "        test_ids = shuffled_indices[start:stop]\n",
        "        train_ids = np.concatenate((shuffled_indices[:start], shuffled_indices[stop:]))\n",
        "        test_ids_per_fold.append(test_ids)\n",
        "        train_ids_per_fold.append(train_ids)\n",
        "        current = stop\n",
        "\n",
        "    return train_ids_per_fold, test_ids_per_fold"
      ],
      "metadata": {
        "id": "RrNpv3pkmAlv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3(b) : Implement the `train_models_and_calc_scores_for_n_fold_cv` function\n",
        "\n",
        "This function will use the procedure from 3(a) to determine the different \"folds\", and then train a separate model at each fold and return that model's training error and heldout error."
      ],
      "metadata": {
        "id": "hqxbRv9VmOmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_models_and_calc_scores_for_n_fold_cv(\n",
        "        estimator, x_NF, y_N, n_folds=3, random_state=0):\n",
        "    ''' Perform n-fold cross validation for a specific sklearn estimator object\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "    estimator : any regressor object with sklearn-like API\n",
        "        Supports 'fit' and 'predict' methods.\n",
        "    x_NF : 2D numpy array, shape (n_examples, n_features) = (N, F)\n",
        "        Input measurements (\"features\") for all examples of interest.\n",
        "        Each row is a feature vector for one example.\n",
        "    y_N : 1D numpy array, shape (n_examples,)\n",
        "        Output measurements (\"responses\") for all examples of interest.\n",
        "        Each row is a scalar response for one example.\n",
        "    n_folds : int\n",
        "        Number of folds to divide provided dataset into.\n",
        "    random_state : int or numpy.RandomState instance\n",
        "        Allows reproducible random splits.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_error_per_fold : 1D numpy array, size n_folds\n",
        "        One entry per fold\n",
        "        Entry f gives the error computed for train set for fold f\n",
        "    test_error_per_fold : 1D numpy array, size n_folds\n",
        "        One entry per fold\n",
        "        Entry f gives the error computed for test set for fold f\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    # Create simple dataset of N examples where y given x\n",
        "    # is perfectly explained by a linear regression model\n",
        "    >>> N = 101\n",
        "    >>> n_folds = 7\n",
        "    >>> x_N3 = np.random.RandomState(0).rand(N, 3)\n",
        "    >>> y_N = np.dot(x_N3, np.asarray([1., -2.0, 3.0])) - 1.3337\n",
        "    >>> y_N.shape\n",
        "    (101,)\n",
        "\n",
        "    >>> import sklearn.linear_model\n",
        "    >>> my_regr = sklearn.linear_model.LinearRegression()\n",
        "    >>> tr_K, te_K = train_models_and_calc_scores_for_n_fold_cv(\n",
        "    ...                 my_regr, x_N3, y_N, n_folds=n_folds, random_state=0)\n",
        "\n",
        "    # Training error should be indistiguishable from zero\n",
        "    >>> np.array2string(tr_K, precision=8, suppress_small=True)\n",
        "    '[0. 0. 0. 0. 0. 0. 0.]'\n",
        "\n",
        "    # Testing error should be indistinguishable from zero\n",
        "    >>> np.array2string(te_K, precision=8, suppress_small=True)\n",
        "    '[0. 0. 0. 0. 0. 0. 0.]'\n",
        "    '''\n",
        "    train_error_per_fold = np.zeros(n_folds, dtype=np.float32)\n",
        "    test_error_per_fold = np.zeros(n_folds, dtype=np.float32)\n",
        "\n",
        "    # TODO define the folds here by calling your function\n",
        "    # e.g. ... = make_train_and_test_row_ids_for_n_fold_cv(...)\n",
        "\n",
        "    # TODO loop over folds and compute the train and test error\n",
        "    # for the provided estimator\n",
        "\n",
        "    # Define the folds here by calling the function\n",
        "    train_ids_per_fold, test_ids_per_fold = make_train_and_test_row_ids_for_n_fold_cv(\n",
        "        n_examples=x_NF.shape[0], n_folds=n_folds, random_state=random_state)\n",
        "\n",
        "    # 폴드별로 모델 학습 및 평가\n",
        "    for fold in range(n_folds):\n",
        "        train_ids = train_ids_per_fold[fold] # 각 폴드에 대해 학습용 데이터와\n",
        "        test_ids = test_ids_per_fold[fold] # 테스트용 데이터 분리\n",
        "\n",
        "        x_train = x_NF[train_ids]\n",
        "        y_train = y_N[train_ids]\n",
        "        x_test = x_NF[test_ids]\n",
        "        y_test = y_N[test_ids]\n",
        "\n",
        "        estimator.fit(x_train, y_train) # 학습용 데이터로 모델 학습\n",
        "\n",
        "        y_train_pred = estimator.predict(x_train) # 학습된 모델을 사용하여 x_train 예측\n",
        "        y_test_pred = estimator.predict(x_test) # # 학습된 모델을 사용하여 x_test 예측\n",
        "\n",
        "        # 예측값과 실체값을 비교하여 MSE 계산\n",
        "        train_error_per_fold[fold] = np.mean((y_train - y_train_pred) ** 2)\n",
        "        test_error_per_fold[fold] = np.mean((y_test - y_test_pred) ** 2)\n",
        "\n",
        "    return train_error_per_fold, test_error_per_fold"
      ],
      "metadata": {
        "id": "QjM_nWEPljn3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test code\n",
        "N = 101\n",
        "n_folds = 7\n",
        "x_N3 = np.random.RandomState(0).rand(N, 3)\n",
        "y_N = np.dot(x_N3, np.asarray([1., -2.0, 3.0])) - 1.3337\n",
        "print(y_N.shape)\n",
        "\n",
        "import sklearn.linear_model\n",
        "my_regr = sklearn.linear_model.LinearRegression()\n",
        "tr_K, te_K = train_models_and_calc_scores_for_n_fold_cv(my_regr, x_N3, y_N, n_folds=n_folds, random_state=0)\n",
        "\n",
        "# Training error should be indistiguishable from zero\n",
        "print(np.array2string(tr_K, precision=8, suppress_small=True))\n",
        "\n",
        "# Testing error should be indistinguishable from zero\n",
        "print(np.array2string(te_K, precision=8, suppress_small=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g_H1a_LmXYx",
        "outputId": "a5b29262-0f8f-4982-a061-2146b2dddcf7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(101,)\n",
            "[ 0.000  0.000  0.000  0.000  0.000  0.000  0.000]\n",
            "[ 0.000  0.000  0.000  0.000  0.000  0.000  0.000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ans 3: 코드 설명\n",
        "이 코드는  주어진 데이터셋과 모델을 사용하여 n-폴드 교차 검증을 수행한다. 또한, 테스트 코드를 통해 각 폴드마다 학습 오류와 테스트 오류를 계산하여 출력한다.\n",
        "###     `make_train_and_test_row_ids_for_n_fold_cv`\n",
        "- 데이터셋을 n개의 폴드로 나누어 교차 검증을 수행할 수 있도록 학습용 데이터와 테스트용 데이터의 인덱스를 생성한다.\n",
        "- 각 폴드마다 테스트 데이터는 다른 인덱스를 가지며, 모든 폴드의 학습 데이터와 테스트 데이터의 결합은 전체 데이터셋을 포함\n",
        "\n",
        "###     `train_models_and_calc_scores_for_n_fold_cv`\n",
        "- 주어진 데이터셋을 여러 개의 폴드로 나누어 각 폴드마다 모델을 학습하고 평가하는 교차 검증을 수행\n",
        "- 앞서 구현한 `make_train_and_test_row_ids_for_n_fold_cv` 함수를 사용하여 폴드를 나누고 각 폴드마다 학습과 평가를 진행\n",
        "\n",
        "### 테스트 결과분석\n",
        "- 각 폴드에 대한 학습 오류와 테스트 오류를 출력\n",
        "- 데이터셋이 선형 회귀 모델로 완벽하게 설명되므로, 오류값은 0에 가깝게 출력되는 것을 볼 수 있다."
      ],
      "metadata": {
        "id": "QSz2nCuRn6-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original resource: [Introductions to Machine Learning, tufts](https://www.cs.tufts.edu/comp/135/2023f/)\n"
      ],
      "metadata": {
        "id": "In4QW7sJe2YQ"
      }
    }
  ]
}